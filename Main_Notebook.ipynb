{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages and funcions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from tweets_utils import tweet_to_tensor, get_vocabulary, process_tweet, split_tweets\n",
    "from data_generators import data_generator,train_generator ,val_generator ,test_generator\n",
    "from model import classifier\n",
    "from train_eval import train_model, evaluate_model, inference\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download tweets and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords length is: 179   -->    Samples ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/faris.almalik/Desktop/NLPCourse...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/faris.almalik/Desktop/NLPCourse...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('twitter_samples', download_dir='/home/faris.almalik/Desktop/NLPCourse')\n",
    "nltk.download('stopwords', download_dir='/home/faris.almalik/Desktop/NLPCourse')\n",
    "stopwords_english = stopwords.words('english')\n",
    "print(f'Stopwords length is: {len(stopwords_english)}   -->    Samples {stopwords_english[:10]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split tweets into training/validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive tweets = 5000 \t Number of negative tweets = 5000\n",
      "Training tweets: 4000 Positive tweets and 4000 Negative tweets\n",
      "Validation tweets: 1000 Positive tweets and 1000 Negative tweets\n",
      "Number of training tweets: 8000\n",
      "Number of validation tweets: 2000\n"
     ]
    }
   ],
   "source": [
    "tweets_train, tweets_val, tweets_train_labels , tweets_val_labels, positive_train ,negative_train, positive_val, negative_val,positive_tweets ,negative_tweets  = split_tweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the tweets and prepare them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet before precessing  -> \t @MsKristinKreuk Hugs ang Kisses from the philippines :)\n",
      "tweet after precessing ->  \t ['hug', 'ang', 'kiss', 'philippin', ':)']\n"
     ]
    }
   ],
   "source": [
    "#Example of a tweet after proccessing. \n",
    "print(f'tweet before precessing  -> \\t {tweets_train[800]}')\n",
    "print(f'tweet after precessing ->  \\t {process_tweet(tweets_train[800])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create vocabulary from the training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary is 9089\n"
     ]
    }
   ],
   "source": [
    "#Get vocabulary \n",
    "vocab = get_vocabulary(train_tweets= tweets_train)\n",
    "print(f'The size of the vocabulary is {len(vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, each tweet should be represented by an array of numbers. To do so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet in strings -> \t @ArianeBeeston Communal knowledge! :)\n",
      "tweet in strings after preprocessing -> \t ['commun', 'knowledg', ':)']\n",
      "tweet in numbers -> \t [7, 2100, 9]\n"
     ]
    }
   ],
   "source": [
    "#Use the teweet_to_tensor function \n",
    "print(f'tweet in strings -> \\t {tweets_train[1000]}')\n",
    "print(f'tweet in strings after preprocessing -> \\t {process_tweet(tweets_train[1000])}')\n",
    "print(f'tweet in numbers -> \\t {tweet_to_tensor(tweets_train[1000], vocabulary= vocab)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the model, we need batch of tweets. Hence, the batches are obtained by: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([64, 27])\n",
      "Targets shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "#Get one batch and check the dimensions \n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "loop = True\n",
    "inputs, targets, example_weights = next(data_generator(data_pos= positive_tweets, data_neg= negative_tweets, batch_size=batch_size, shuffle=shuffle, loop = loop,vocab_dict= vocab))\n",
    "print(f'Inputs shape: {inputs.shape}')\n",
    "print(f'Targets shape: {targets.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model, define loaders, and define optimizer/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate model instant\n",
    "model = classifier(vocab_size=len(vocab))\n",
    "\n",
    "#Define some parameters \n",
    "batch_size = 64\n",
    "lr = 0.0001\n",
    "epochs = 10\n",
    "\n",
    "#Instantiate loss function and optimizer\n",
    "optimizer = torch.optim.Adam(params = model.parameters() ,lr = lr)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#Create train/test/val loaders \n",
    "train_loader = train_generator(batch_size= batch_size, train_pos= positive_train, train_neg=negative_train,vocab_dict=vocab, loop = True)\n",
    "val_loader = val_generator(batch_size= batch_size, val_pos= positive_val, val_neg=negative_val, vocab_dict=vocab, loop= True)\n",
    "test_loader = test_generator(batch_size= batch_size, val_pos= positive_val, val_neg=negative_val, vocab_dict=vocab, loop= False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs : 1\n",
      "Steps  = 125 \t loss = 0.6442 \t Acc = 0.7790\n",
      "==============================================\n",
      "epochs : 2\n",
      "Steps  = 125 \t loss = 0.4772 \t Acc = 0.9380\n",
      "==============================================\n",
      "epochs : 3\n",
      "Steps  = 125 \t loss = 0.3080 \t Acc = 0.9566\n",
      "==============================================\n",
      "epochs : 4\n",
      "Steps  = 125 \t loss = 0.2025 \t Acc = 0.9674\n",
      "==============================================\n",
      "epochs : 5\n",
      "Steps  = 125 \t loss = 0.1436 \t Acc = 0.9764\n",
      "==============================================\n",
      "epochs : 6\n",
      "Steps  = 125 \t loss = 0.1082 \t Acc = 0.9831\n",
      "==============================================\n",
      "epochs : 7\n",
      "Steps  = 125 \t loss = 0.0849 \t Acc = 0.9871\n",
      "==============================================\n",
      "epochs : 8\n",
      "Steps  = 125 \t loss = 0.0691 \t Acc = 0.9885\n",
      "==============================================\n",
      "epochs : 9\n",
      "Steps  = 125 \t loss = 0.0580 \t Acc = 0.9892\n",
      "==============================================\n",
      "epochs : 10\n",
      "Steps  = 125 \t loss = 0.0499 \t Acc = 0.9900\n",
      "==============================================\n"
     ]
    }
   ],
   "source": [
    "#train model \n",
    "trained_model = train_model(model = model, train_loader= train_loader, optimizer= optimizer, criterion= criterion, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained model's performance on the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.0571 \t Acc = 0.9884\n"
     ]
    }
   ],
   "source": [
    "#evaluate model \n",
    "evaluate_model(model= trained_model, test_loader= test_loader, criterion= criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, Try it! Enter any tweet and the model will tell whether its positive or negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Sentiment\n"
     ]
    }
   ],
   "source": [
    "my_tweeet = input('Write your tweet and press enter \\n')\n",
    "inference(tweet= my_tweeet, vocab= vocab, model= trained_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('transformer-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37906e3799e3dad222cfed0447967870e65218c8bdbf06cfd0739074c03b6910"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
